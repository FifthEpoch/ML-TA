{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5bad44c",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "\n",
    "What are our learning objectives for this lesson?\n",
    "\n",
    "* Learn about the Perceptron achitecture\n",
    "* Learn how to train a Perceptron model \n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Marsland, Stephen. Machine Learning: An Algorithmic Perspective 2nd ed. (2015).\n",
    "\n",
    "## Lab Tasks\n",
    "\n",
    "1. Initialize weight vector $\\vec{w}$ with random values\n",
    "2. Implement an activation function\n",
    "3. Implement an update function\n",
    "4. Train the Perceptron model on the OR operation\n",
    "\n",
    "### Initialize a weight vector\n",
    "\n",
    "#### What is the dimensionality of this vector?\n",
    "\n",
    "Let's first think through the dimensionality of our weight vector. How many entries should it have? Recall that the sum $h$ is given by\n",
    "\n",
    "$$h = \\sum_{i=1}^{m}w_i x_i $$\n",
    "\n",
    "where $w_i$ is the $i^{th}$ entry of the weight vector $\\vec{w}$ and $x_i$ is the $i^{th}$ entry of the input vector $\\vec{x}$. This means that for every input value we will need a weight value that goes with it. In another word, if there are $n$ entries in our input vector $\\vec{x}$, we will need a weight vector $\\vec{w}$ of length $n$. \n",
    "\n",
    "Recall that we want to include an additional bias node in our model where the input is always a non-zero value. Since we will be alterning the weight $w_b$ associated with the bias node at the same time as all other nodes in our network, it might be convinient to dedicate a spot in our weight vector $\\vec{w}$ that represents the weight $w_0$. Thus, for a problem with $n$ inputs, $\\vec{w}$ might look something like this:\n",
    "\n",
    "$$\\vec{w}^T = [w_0, w_1, w_2, ..., w_n]$$\n",
    "\n",
    "Notice that $\\vec{w}$ has a length of $n+1$. \n",
    "\n",
    "\n",
    "#### What initial value do we assign the vector?\n",
    "\n",
    "The initial value we assign the entries in our weight vector $\\vec{w}$ matters. If we assign very large values, then $h$ will also be a very large value regardless of our input values (given that input values are not all zeros). If we assign extremely small values, we run into the opposite problem, where $h$ will also be a extremely small value regardless of the input values. We also want to avoid initializing it with all zeros. If you're interested in understanding why, please take a look at this [stackexcahnge response](https://datascience.stackexchange.com/questions/26134/initialize-perceptron-weights-with-zero) that gives a concise explanation. \n",
    "\n",
    "In conclusion, it is ideal to have weights that are neither too big or too small. For neural networks (which we will get to soon), The last desired quaility in our initialization values would be non-uniformity. We don't want to assign a single value across the board, and the way to achieve this is by using pseudorandom generators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2029ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight vector:\n",
      "[[0.46386536]\n",
      " [0.2929082 ]\n",
      " [0.17391119]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# define inputs\n",
    "inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "targets = np.array([[0],[1],[1],[1]])\n",
    "\n",
    "# capture how many nodes are needed by examining\n",
    "# how many entries is in a set of input\n",
    "n_in = np.shape(inputs)[1]\n",
    "        \n",
    "# how many values are we expecting as output?\n",
    "# in this case, we want a single value as our output\n",
    "n_out = 1\n",
    "        \n",
    "# η is the rate of which we learn from our data\n",
    "# typically, 0.1 < η < 0.4\n",
    "eta = 0.2\n",
    "        \n",
    "# TODO: initialize a weight vector\n",
    "# how long does it have to be?\n",
    "# what initial values would you assign it?\n",
    "# self.weights = ???\n",
    "        \n",
    "weights = np.random.rand(n_in+1, n_out) * (1 / math.sqrt(n_in+1))\n",
    "        \n",
    "print(f'weight vector:\\n{weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebfd92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 0 1]\n",
      " [1 1 0]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# insert a bias value of 1 into the inputs\n",
    "def insert_bias(_inputs):    \n",
    "    return np.insert(_inputs, 0, 1, axis=1)\n",
    "\n",
    "inputs = insert_bias(inputs)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9097c9",
   "metadata": {},
   "source": [
    "### Implement an activation function\n",
    "\n",
    "The activation of each neuron $j$ using activation function $g$ is given by:\n",
    "\n",
    "$$y_j = g(\\sum_{i=0}^m w_{ij}x_i)=\\begin{cases}1 \\quad \\text{  if  }g(\\sum_{i=0}^m w_{ij}x_i)>0 \\\\ 0 \\quad \\text{  if  } g(\\sum_{i=0}^m w_{ij}x_i) \\leq 0 \\end{cases}$$\n",
    "\n",
    "Implement an activation function according to the above equation. Here are several things to consider:\n",
    "* What parameters do you need to pass to this function?\n",
    "* How could you use numpy to compute $\\vec{y}$ instead of writing a for loop to compute each $y_j$ individually? \n",
    "* What kind of object is this function going to return?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee45dbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: complete this function\n",
    "def calculate_activations(_inputs, _weights):\n",
    "    \n",
    "    _activations = np.dot(_inputs, _weights)\n",
    "    \n",
    "    return np.where(_activations>0,1,0)\n",
    "\n",
    "test_weights = np.random.rand(n_in+1, n_out) * (1 / math.sqrt(n_in+1))\n",
    "test_activations = calculate_activations(inputs, test_weights)\n",
    "print(test_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b26a4",
   "metadata": {},
   "source": [
    "### Update the weight vector\n",
    "\n",
    "Let $y_j$ be the output of the $j^{th}$ neuron, and $t_j$ the target for that neuron, and $\\eta$ (eta) the learning rate. \n",
    "\n",
    "$$w_{ij} \\leftarrow w_{ij} - \\eta(y_j - t_j) \\cdot x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f037f25e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3y/zpgzwchj28qcmhh3ph_tf4t40000gn/T/ipykernel_19087/2812131284.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtest_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: complete this function\n",
    "def update_weights(_inputs, _targets, _activations, _weights):\n",
    "    \n",
    "    _weights -= eta*np.dot(np.transpose(_inputs),_activations-_targets)\n",
    "    \n",
    "    print(f'error:\\n{_activations-_targets}')\n",
    "    print(f'inputs transposed:\\n{np.transpose(_inputs)}')    \n",
    "    print(f'weight vector:\\n{_weights}')\n",
    "    \n",
    "    return _weights\n",
    "\n",
    "test_weights = update_weights(inputs, targets, test_activations, test_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451bc11",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Now that you have imeplemented the ```calculate_activations``` function and ```update_weights``` function, we can start training the Perceptron model. There are 3 steps in training:\n",
    "1. calculate the activation based on the current weights.\n",
    "2. update the weight vector based on the gradient of the error for this time step.\n",
    "3. repeat for $n$ iterations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80a20c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 1\n",
      "error:\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "inputs transposed:\n",
      "[[1 1 1 1]\n",
      " [0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "weight vector:\n",
      "[[0.26386536]\n",
      " [0.2929082 ]\n",
      " [0.17391119]]\n",
      "\n",
      "iteration: 2\n",
      "error:\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "inputs transposed:\n",
      "[[1 1 1 1]\n",
      " [0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "weight vector:\n",
      "[[0.06386536]\n",
      " [0.2929082 ]\n",
      " [0.17391119]]\n",
      "\n",
      "iteration: 3\n",
      "error:\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "inputs transposed:\n",
      "[[1 1 1 1]\n",
      " [0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "weight vector:\n",
      "[[-0.13613464]\n",
      " [ 0.2929082 ]\n",
      " [ 0.17391119]]\n",
      "\n",
      "iteration: 4\n",
      "error:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "inputs transposed:\n",
      "[[1 1 1 1]\n",
      " [0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "weight vector:\n",
      "[[-0.13613464]\n",
      " [ 0.2929082 ]\n",
      " [ 0.17391119]]\n",
      "\n",
      "iteration: 5\n",
      "error:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "inputs transposed:\n",
      "[[1 1 1 1]\n",
      " [0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "weight vector:\n",
      "[[-0.13613464]\n",
      " [ 0.2929082 ]\n",
      " [ 0.17391119]]\n"
     ]
    }
   ],
   "source": [
    "iterations = 5\n",
    "\n",
    "for i in range(iterations):\n",
    "    print(f'\\niteration: {i+1}')\n",
    "    activations = calculate_activations(inputs, weights)\n",
    "    weights = update_weights(inputs, targets, activations, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245189f",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "Now we feed the trained model a test set to see how accurate the model is.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1691a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "test = np.array([[1,0],[1,1],[0,1],[0,0]])\n",
    "test_target = np.array([[1],[1],[1],[0]])\n",
    "test = insert_bias(test)\n",
    "activations = calculate_activations(test, weights)\n",
    "\n",
    "# check the accuracy of your trained model\n",
    "accuracy = 0.0\n",
    "for i in range(len(test_target)):\n",
    "    if test_target[i] == activations[i]: accuracy += 1 \n",
    "print(f'accuracy: {(accuracy / len(test_target))*100.0}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08506300",
   "metadata": {},
   "source": [
    "### Bonus Task\n",
    "\n",
    "Train your model to learn the XOR operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db9988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
