{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b809308",
   "metadata": {},
   "source": [
    "## Racetrack with Monte Carlo Off-policy control\n",
    "\n",
    "In this lab, we will implement the algorithm shown on p.111 of the textbook–Off Policy MC Control, for estimating $\\pi^*$. Below is the problem as shown in the textbook:\n",
    "\n",
    "* Consider driving a race car around a turn like those shown in Figure 5.5. You want to go as fast as possible, but not so fast as to run off the track. In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments to the velocity components. Each may be changed by +1, -1, or 0 in each step, for a total of nine (3 x 3) actions. Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line. Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line. The rewards are -1 for each step until the car crosses the finish line. If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car’s location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the finish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability 0.1 at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy (but turn the noise off for these trajectories).\n",
    "\n",
    "To complete this exercise, we first recognize that there are three parts involved:\n",
    "1. Setting up the environment\n",
    "2. Creating the car agent capable of MC control\n",
    "3. Train the agent and visualize outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be456ddb",
   "metadata": {},
   "source": [
    "### Setting Up the Environment\n",
    "\n",
    "We want to create a grid with a defined area that is our racetrack. We also want to define areas where the start and finish lines are. Anytime when the car agent is out of bound, it is returned to a random position in the start line and the episode continues. \n",
    "\n",
    "Below shows a skeleton of a track that is made to model one of the tracks shown in figure 5.5 in the textbook. There are a couple of options in terms of setting up the environment. You may choose to complete the Track class by filling in the functions intentionally left blank, you could toss the template code and build a track environment to your own likings, or you could use the track environment provided by the ```gym``` Python module. \n",
    "\n",
    "Note that you will have to install ```gym``` and import it to this lab if you choose to use ```gym```, and you will need to integrate the control algorithm we are building in this lab with the environment.\n",
    "\n",
    "\n",
    "#### Task 1: Complete the functions in the Track class\n",
    "\n",
    "\n",
    "The ```Track``` class is equipped with some basic functions such as setting up and visualizing a track. The learning agent can interact with the track to obtain information like: \n",
    "    * the state of the cell it is in\n",
    "        * the states are 'track', 'oob', 'start', 'finish'\n",
    "    * a random coordinate for a start state\n",
    "        * every time the agent goes off track, it will need to return to a randomized position on the starting line\n",
    "    * any other functions you feel would make sense as part of the environment (e.g. if the agent has crossed the finish line or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b563a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0196d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    \n",
    "  def __init__(self, _h, _w):\n",
    "    self.w = _w\n",
    "    self.h = _h\n",
    "    # set each cell in grid as 'track'\n",
    "    self.grid = np.array([['track'] * _w for _ in range(_h)], dtype='U8')\n",
    "\n",
    "  def make_track_1(self):\n",
    "    \"\"\"\n",
    "    sets the track on the left in figure 5.5 in textbook\n",
    "    \"\"\"\n",
    "    # set start line\n",
    "    self.grid[-1,:] = 'start'\n",
    "    # set finish line\n",
    "    self.grid[:,-1] = 'finish'\n",
    "    # set area to be out of bound\n",
    "    self.grid[:,0] = 'oob'\n",
    "    self.grid[0, :] = 'oob'\n",
    "    self.grid[:5,1] = 'oob'\n",
    "    self.grid[:4,2] = 'oob'\n",
    "    self.grid[:2,3] = 'oob'\n",
    "    self.grid[-18:,1] = 'oob'\n",
    "    self.grid[-10:,2] = 'oob'\n",
    "    self.grid[-7:,3] = 'oob'\n",
    "    self.grid[7:,11:] = 'oob'\n",
    "    self.grid[8:,10] = 'oob'\n",
    "    \n",
    "    # so that row 0 is start line\n",
    "    self.grid = np.flip(self.grid, axis=0)\n",
    "    \n",
    "  def show_grid(self, _show_trajectory=False, _ys=[], _xs=[]):\n",
    "    \"\"\"\n",
    "    visualize the track with cells in  \n",
    "    colors that coresponse to the cell type\n",
    "    \"\"\"\n",
    "    flipped_grid = np.flip(self.grid, axis=0)\n",
    "    # setting up the colors that goes with cell type\n",
    "    self.color_dict = {'track': 'white', \n",
    "                       'oob': 'silver', \n",
    "                       'start': 'red', \n",
    "                       'finish': 'green'}\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(6, 10)\n",
    "    ax.set_xlim(-1/(self.w), self.w+1/(self.w))\n",
    "    ax.set_ylim(-1/(self.h), self.h+1/(self.h))\n",
    "    ax.set_axis_off()\n",
    "    # use the table and cell objects to make grid\n",
    "    table = Table(ax)\n",
    "    # table = Table(ax, bbox=[0, 0, 1, 1])\n",
    "    for (i, j), cell in np.ndenumerate(self.grid):\n",
    "        table.add_cell(\n",
    "            i, j, 1/self.w, 1/self.h, fill=True, \n",
    "            facecolor=self.color_dict[flipped_grid[i][j]]\n",
    "        )\n",
    "    ax.add_table(table)\n",
    "    if _show_trajectory:\n",
    "        xs = _xs + 0.5\n",
    "        ys = _ys + 0.5\n",
    "        plt.plot(xs, ys, '-')\n",
    "    plt.show()\n",
    "    \n",
    "  def get_cell_state(self, _y, _x):\n",
    "    \"\"\"\n",
    "    returns the state of the cell [_y, _x]\n",
    "            ('track', 'oob', 'start', 'finish')\n",
    "    \"\"\"\n",
    "    if _y < 0 or _y >= self.h or _x < 0 or _x >= self.w: \n",
    "        if self.h > _y > self.h-8 and _x >= self.w: \n",
    "            return 'finish'\n",
    "        return 'oob'\n",
    "    return self.grid[_y, _x]\n",
    "    \n",
    "    \n",
    "  def randomized_start(self):\n",
    "    \"\"\"\n",
    "    this function makes the assumption that \n",
    "    the start line will always be on the bottom row\n",
    "    ---\n",
    "    returns a tuple that represents a randomized \n",
    "            position on the start line\n",
    "    \"\"\"\n",
    "    state = 'oob'\n",
    "    while state != 'start':\n",
    "        rand_i = np.random.randint(self.w)\n",
    "        state = self.grid[0, rand_i]\n",
    "    return np.array([0, rand_i]), np.array([0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0abcd4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's take a look at our environment\n",
    "H = 33\n",
    "W = 18\n",
    "track = Track(H, W)\n",
    "track.make_track_1()\n",
    "track.show_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a50f4b",
   "metadata": {},
   "source": [
    "### Creating the Car Agent\n",
    "\n",
    "We want to give the car agent access to neccessary information such as:\n",
    "* Its position on the grid\n",
    "* Its velocity\n",
    "* The availble actions and their constraints \n",
    "    * actions are +1, -1, and 0 in each direction on each step\n",
    "        * this translates to 9 actions: $[-1, -1], [-1, 0], [-1, 1], [0, -1], [0, 0], [0, 1], [1, -1], [1, 0], [1, -1]$\n",
    "        * determine the dimension of the state space and state-action space so that the matrices for Q, C, pi, etc are implemented correctly\n",
    "    * velocity components must be nonnegative and less than 5, and annot both be zero except at the starting line\n",
    "        * how would you handle checking the bound?\n",
    "        * if you had to correct the velocity because of the agent's action, do you record the action that was actually choosen or record the corrected action as if the agent knew about the constraint on its velocity? \n",
    "\n",
    "### Implementing the Learning Algorithm\n",
    "\n",
    "We can also implement off-policy Monte Carlo control algorithm in this next code block. For Monte Carlo, we want to document for each episode, for each time step $t$ the state $S_t$, the action taken $A_t$, and the reward the eagent received after transitioning to the next state $R_{t+1}$. We will only update $Q$ after the agent reaches the terminal state, in our case, the finish line.\n",
    "\n",
    "Since we want to implement an off-policy version of Monte Carlo, we also want to set up a policy matrix $\\pi$ so that we can iterate on it based on the learning from finishing each episode. Note that the agent will not be following $\\pi$ during the episode, and will instead follow a behavioral policy $b$, which is set to be any soft-policy in the textbook. Soft-policies can provide _coverage_, a property meaning that any actions taken under $\\pi$ will be taken under $b$ with a non-zero probability. \n",
    "\n",
    "Below is a template for the ```Agent``` class with a couple of suggested functions. Feel free to complete the functions, or write other functions to complete the neccessary structures for the learning process.\n",
    "\n",
    "#### Task 2: Complete the functions in the Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df7fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, _env, _eps=0.01, _gamma=0.9):\n",
    "        \n",
    "        self.env = _env\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.eps = _eps\n",
    "        self.gamma = _gamma\n",
    "        \n",
    "        # agent's position: [y, x]\n",
    "        self.pos = np.array([0, 0])\n",
    "        self.cell_state = 'start'\n",
    "        # agent's velocity: [velocity along y-axis, velocity along x-axis]\n",
    "        self.v = np.array([0, 0])\n",
    "        \n",
    "        # 9 actions available\n",
    "        self.actions = np.array([[i, j] for j in range(-1, 2) for i in range(-1, 2)])\n",
    "        # set limits on velocity\n",
    "        self.min_v = 1\n",
    "        self.max_v = 4\n",
    "        # state-action space dim\n",
    "        self.q_dim = (self.env.h, self.env.w, self.max_v, self.max_v, len(self.actions))\n",
    "        self.s_dim = (self.env.h, self.env.w, self.max_v, self.max_v)\n",
    "        # Q is a 3D array\n",
    "        self.q = np.full(self.q_dim, -1000) # arbitrary initialization value\n",
    "                                              # another arbitrary initialization: np.random.rand()*(1/len(self.actions))\n",
    "        # record keeping\n",
    "        self.c = np.zeros(self.q_dim) # used to track cummulative reward\n",
    "        self.r_rec = [] # keep a record of rewards\n",
    "        self.s_rec = [] # keep a record of visited positions\n",
    "        self.a_rec = [] # keep a record of actions taken\n",
    "        \n",
    "        # initialize a policy b\n",
    "        self.pi = self.generate_policy()\n",
    "        \n",
    "    \n",
    "    def clear_records(self):\n",
    "        \"\"\"\n",
    "        resets data structures at the beginning of an episode\n",
    "        \"\"\"\n",
    "        self.pos, self.v = self.env.randomized_start()\n",
    "        self.cell_state = 'start'\n",
    "        self.r_rec = [] \n",
    "        self.s_rec = [] \n",
    "        self.a_rec = []\n",
    "    \n",
    "    def generate_policy(self):\n",
    "        \"\"\"\n",
    "        generate pi based on Q\n",
    "        \"\"\"\n",
    "        pi = np.full(self.s_dim, -1)\n",
    "        for y in range(self.env.h):\n",
    "            for x in range(self.env.w):\n",
    "                for vy in range(self.max_v):\n",
    "                    for vx in range(self.max_v):\n",
    "                        pi[y, x, vy, vx] = self.get_argmax(\n",
    "                            self.q[y, x, vy, vx])\n",
    "        return pi\n",
    "    \n",
    "    def get_argmax(self, _qs):\n",
    "        \"\"\"\n",
    "        implement argmax for determining pi \n",
    "        ---\n",
    "        returns the action with maximum expected value\n",
    "        \"\"\"\n",
    "        max_as = np.where(_qs == _qs.max())[0]\n",
    "        \n",
    "        # below shows a solution where ties are broken randomly\n",
    "        # instead of breaking ties consistantly\n",
    "        return max_as[np.random.randint(len(max_as))]\n",
    "        \n",
    "        \n",
    "    \n",
    "    def find_possible_actions(self, _v):\n",
    "        \"\"\"\n",
    "        gather actions that will not cause velocity  \n",
    "        to be more than 4, or negative, or both zeros\n",
    "        \"\"\"\n",
    "        possible_actions = []\n",
    "        for a_i in range(len(self.actions)):\n",
    "            new_v = _v + self.actions[a_i]\n",
    "            if new_v.any() and (new_v < 5).all() and (new_v >= 0).all():\n",
    "                possible_actions.append(a_i)\n",
    "        return np.array(possible_actions, dtype=int)\n",
    "        \n",
    "    \n",
    "    def epsilon_greedy(self):\n",
    "        \"\"\"\n",
    "        implement a soft behavioral policy b \n",
    "        using epsilon-greedy\n",
    "        ---\n",
    "        returns an action\n",
    "        \"\"\"\n",
    "        possible_actions = self.find_possible_actions(self.v)\n",
    "        if np.random.rand() >= self.eps:\n",
    "            qs = self.q[self.pos[0], self.pos[1], \n",
    "                        self.v[0]-1, self.v[1]-1]\n",
    "            qs = qs[possible_actions]\n",
    "            return possible_actions[self.get_argmax(qs)]\n",
    "        else:\n",
    "            a_i = np.random.randint(len(possible_actions))\n",
    "            return possible_actions[a_i]\n",
    "    \n",
    "    \n",
    "    def get_softmax(self, _y, _x, _vy, _vx):\n",
    "        \"\"\"\n",
    "        implement a soft behavioral policy b \n",
    "        ---\n",
    "        returns the action the agent is taking\n",
    "        \"\"\"\n",
    "        # using softmax as the soft behavioral policy b\n",
    "        softmax = np.exp(self.q[_y][_x][_vy][_vx])\n",
    "        softmax /= np.sum(softmax)\n",
    "        return softmax\n",
    "    \n",
    "    \n",
    "    def get_next_pos(self):\n",
    "        \"\"\"\n",
    "        accepts an action that is a np array [y, x]\n",
    "        set the new position with the velocity\n",
    "        \"\"\"\n",
    "        return self.pos + self.v\n",
    "    \n",
    "                                \n",
    "    def get_new_v(self, _v, _action):\n",
    "        \"\"\"\n",
    "        get new velocity from  \n",
    "        current velocity and the choosen action\n",
    "        \"\"\"\n",
    "        return _v + _action\n",
    "    \n",
    "                                \n",
    "    def mc_step(self):\n",
    "        \"\"\"\n",
    "        handles everything that needs to happen and store\n",
    "        in a single time step for Monte Carlo control\n",
    "        \"\"\"\n",
    "        # store position of agent\n",
    "        self.s_rec.append([self.pos[0], self.pos[1], \n",
    "                      self.v[0]-1, self.v[1]-1])\n",
    "        a_index = self.epsilon_greedy()            \n",
    "        \n",
    "        # store action of choice before uncertainty is introduced\n",
    "        self.a_rec.append([a_index])\n",
    "        \n",
    "        # with probability 0.1 at each time step the velocity increments \n",
    "        # are both zero, independently of the intended increments.\n",
    "        if np.random.rand() < 0.0: a_index = 4\n",
    "        \n",
    "        # get new velocity and position\n",
    "        self.v = self.get_new_v(self.v, self.actions[a_index])\n",
    "        self.pos = self.get_next_pos()\n",
    "        \n",
    "        # get the cell state of new position    \n",
    "        self.cell_state = self.env.get_cell_state(self.pos[0], self.pos[1])\n",
    "        \n",
    "        # retrieve and store reward\n",
    "        r = -1 if self.cell_state != 'finish' else 0\n",
    "        self.r_rec.append(r)\n",
    "                                \n",
    "        # handle out of bound                        \n",
    "        if self.cell_state == 'oob':\n",
    "            self.pos, self.v = self.env.randomized_start()\n",
    "        \n",
    "    \n",
    "    def offpolicy_mc_control(self, _episode=1000):\n",
    "        \"\"\"\n",
    "        generates episodes and iteratively updates policy \n",
    "        \"\"\"\n",
    "        t_avg = 0\n",
    "        for episode in range(_episode):\n",
    "            t = 0\n",
    "            self.clear_records()\n",
    "            while self.cell_state != 'finish':\n",
    "                self.mc_step()\n",
    "                t += 1\n",
    "                t_avg += 1\n",
    "            if t <= 10:\n",
    "                s_rec_np = np.array(self.s_rec)        \n",
    "                self.env.show_grid(_show_trajectory=True, \n",
    "                                   _ys=s_rec_np[:,0], \n",
    "                                   _xs=s_rec_np[:,1])\n",
    "            if episode % 10000 == 0 and episode != 0:\n",
    "                print(f'episode {episode-10000}-{episode} reached terminal at t: {t_avg/10000}')\n",
    "                t_avg = 0\n",
    "                \n",
    "            G = 0\n",
    "            W = 1\n",
    "            for t in reversed(range(len(self.r_rec)-1)):\n",
    "                G = self.gamma * G + (self.r_rec[t+1])\n",
    "                s_t = self.s_rec[t]\n",
    "                a_t = self.a_rec[t]\n",
    "                # increment c for state-action pair    \n",
    "                self.c[tuple(s_t + a_t)] += W\n",
    "                # make update to the expected value of state-action pair    \n",
    "                c_t = self.c[tuple(s_t + a_t)]\n",
    "                q_t = self.q[tuple(s_t + a_t)]\n",
    "                update_t = (W / c_t) * (G - q_t)\n",
    "                self.q[tuple(s_t + a_t)] += update_t\n",
    "                \n",
    "                # calculate b(A_t|S_t)\n",
    "                b_as = self.eps / len(self.actions)\n",
    "                if a_t[0] == self.pi[tuple(s_t)]:\n",
    "                    b_as += (1 - self.eps)\n",
    "                # pick max value from self.q[tuple(s_t)] to update policy\n",
    "                self.pi[tuple(s_t)] = self.get_argmax(self.q[tuple(s_t)])\n",
    "                # break loop conditionally\n",
    "                if a_t[0] != self.pi[tuple(s_t)]: break\n",
    "                # update W\n",
    "                W /= b_as\n",
    "\n",
    "        s_rec_np = np.array(self.s_rec)        \n",
    "        self.env.show_grid(_show_trajectory=True, \n",
    "                           _ys=s_rec_np[:,0], \n",
    "                           _xs=s_rec_np[:,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e12ec",
   "metadata": {},
   "source": [
    "## Traning with Monte Carlo\n",
    "\n",
    "Now that you have set up the agent and the environment, train the agent. Play around with the hyperparameters and the number of episode to see how they impact performance.\n",
    "\n",
    "You can visualize the agent's trajectory by using the ```show_grid()``` function provided in the ```Track``` class. Set the parameter ```_show_trajectory``` to ```True```, and pass in the y components and the x components of the agent's position history with the parameters ```_ys``` and ```_xs```.\n",
    "\n",
    "There might be a lot of bugs in your code at first. A good benchmark is that the first episode typically takes about 1000-2000 time steps to complete, and the subsequent episode should take significantly less time steps to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "170eab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0-10000 reached terminal at t: 19.3303\n",
      "episode 10000-20000 reached terminal at t: 17.8635\n",
      "episode 20000-30000 reached terminal at t: 18.2624\n",
      "episode 30000-40000 reached terminal at t: 18.0085\n",
      "episode 40000-50000 reached terminal at t: 17.9627\n",
      "episode 50000-60000 reached terminal at t: 17.9224\n",
      "episode 60000-70000 reached terminal at t: 18.2701\n",
      "episode 70000-80000 reached terminal at t: 17.8575\n",
      "episode 80000-90000 reached terminal at t: 18.4229\n",
      "episode 90000-100000 reached terminal at t: 17.7576\n",
      "episode 100000-110000 reached terminal at t: 18.8477\n",
      "episode 110000-120000 reached terminal at t: 18.426\n",
      "episode 120000-130000 reached terminal at t: 17.8509\n",
      "episode 130000-140000 reached terminal at t: 18.0552\n",
      "episode 140000-150000 reached terminal at t: 17.7695\n",
      "episode 150000-160000 reached terminal at t: 19.2433\n",
      "episode 160000-170000 reached terminal at t: 19.9244\n",
      "episode 170000-180000 reached terminal at t: 20.9559\n",
      "episode 180000-190000 reached terminal at t: 19.956\n",
      "episode 190000-200000 reached terminal at t: 20.1715\n",
      "episode 200000-210000 reached terminal at t: 22.5254\n",
      "episode 210000-220000 reached terminal at t: 17.3561\n",
      "episode 220000-230000 reached terminal at t: 16.9869\n",
      "episode 230000-240000 reached terminal at t: 15.5493\n",
      "episode 240000-250000 reached terminal at t: 15.3159\n",
      "episode 250000-260000 reached terminal at t: 15.3269\n",
      "episode 260000-270000 reached terminal at t: 15.8165\n",
      "episode 270000-280000 reached terminal at t: 16.1358\n",
      "episode 280000-290000 reached terminal at t: 15.9925\n",
      "episode 290000-300000 reached terminal at t: 16.09\n",
      "episode 300000-310000 reached terminal at t: 16.0424\n",
      "episode 310000-320000 reached terminal at t: 15.9756\n",
      "episode 320000-330000 reached terminal at t: 15.9955\n",
      "episode 330000-340000 reached terminal at t: 16.0079\n",
      "episode 340000-350000 reached terminal at t: 16.0068\n",
      "episode 350000-360000 reached terminal at t: 16.0391\n",
      "episode 360000-370000 reached terminal at t: 16.02\n",
      "episode 370000-380000 reached terminal at t: 16.0283\n",
      "episode 380000-390000 reached terminal at t: 16.0656\n",
      "episode 390000-400000 reached terminal at t: 16.0848\n",
      "episode 400000-410000 reached terminal at t: 16.0178\n",
      "episode 410000-420000 reached terminal at t: 16.0193\n",
      "episode 420000-430000 reached terminal at t: 15.9376\n",
      "episode 430000-440000 reached terminal at t: 15.9789\n",
      "episode 440000-450000 reached terminal at t: 16.029\n",
      "episode 450000-460000 reached terminal at t: 15.977\n",
      "episode 460000-470000 reached terminal at t: 15.9547\n",
      "episode 470000-480000 reached terminal at t: 15.9716\n",
      "episode 480000-490000 reached terminal at t: 15.9322\n",
      "episode 490000-500000 reached terminal at t: 15.4488\n",
      "episode 500000-510000 reached terminal at t: 15.4403\n",
      "episode 510000-520000 reached terminal at t: 15.2593\n",
      "episode 520000-530000 reached terminal at t: 14.4197\n",
      "episode 530000-540000 reached terminal at t: 14.4496\n",
      "episode 540000-550000 reached terminal at t: 14.5633\n",
      "episode 550000-560000 reached terminal at t: 14.3459\n",
      "episode 560000-570000 reached terminal at t: 14.3416\n",
      "episode 570000-580000 reached terminal at t: 14.3371\n",
      "episode 580000-590000 reached terminal at t: 14.3671\n",
      "episode 590000-600000 reached terminal at t: 14.3268\n",
      "episode 600000-610000 reached terminal at t: 14.338\n",
      "episode 610000-620000 reached terminal at t: 14.313\n",
      "episode 620000-630000 reached terminal at t: 14.3531\n",
      "episode 630000-640000 reached terminal at t: 14.3524\n",
      "episode 640000-650000 reached terminal at t: 14.3162\n",
      "episode 650000-660000 reached terminal at t: 14.3488\n",
      "episode 660000-670000 reached terminal at t: 14.357\n",
      "episode 670000-680000 reached terminal at t: 14.3291\n",
      "episode 680000-690000 reached terminal at t: 14.3679\n",
      "episode 690000-700000 reached terminal at t: 14.3082\n",
      "episode 700000-710000 reached terminal at t: 14.5305\n",
      "episode 710000-720000 reached terminal at t: 14.5048\n",
      "episode 720000-730000 reached terminal at t: 14.3441\n",
      "episode 730000-740000 reached terminal at t: 14.3136\n",
      "episode 740000-750000 reached terminal at t: 14.4304\n",
      "episode 750000-760000 reached terminal at t: 14.4298\n",
      "episode 760000-770000 reached terminal at t: 13.9114\n",
      "episode 770000-780000 reached terminal at t: 13.9174\n",
      "episode 780000-790000 reached terminal at t: 13.8778\n",
      "episode 790000-800000 reached terminal at t: 13.8898\n",
      "episode 800000-810000 reached terminal at t: 13.8519\n",
      "episode 810000-820000 reached terminal at t: 13.8426\n",
      "episode 820000-830000 reached terminal at t: 13.8665\n",
      "episode 830000-840000 reached terminal at t: 13.8641\n",
      "episode 840000-850000 reached terminal at t: 13.8449\n",
      "episode 850000-860000 reached terminal at t: 13.8661\n",
      "episode 860000-870000 reached terminal at t: 13.8412\n",
      "episode 870000-880000 reached terminal at t: 13.8472\n",
      "episode 880000-890000 reached terminal at t: 13.845\n",
      "episode 890000-900000 reached terminal at t: 13.8651\n",
      "episode 900000-910000 reached terminal at t: 13.8196\n",
      "episode 910000-920000 reached terminal at t: 13.8237\n",
      "episode 920000-930000 reached terminal at t: 13.8804\n",
      "episode 930000-940000 reached terminal at t: 13.8322\n",
      "episode 940000-950000 reached terminal at t: 13.9278\n",
      "episode 950000-960000 reached terminal at t: 13.9172\n",
      "episode 960000-970000 reached terminal at t: 13.8658\n",
      "episode 970000-980000 reached terminal at t: 13.858\n",
      "episode 980000-990000 reached terminal at t: 13.8847\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = Agent(track, _eps=0.006, _gamma=0.99)\n",
    "agent.offpolicy_mc_control(_episode=1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
