{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b809308",
   "metadata": {},
   "source": [
    "## Racetrack with Monte Carlo Off-policy control\n",
    "\n",
    "In this lab, we will implement the algorithm shown on p.111 of the textbook–Off Policy MC Control, for estimating $\\pi^*$. Below is the problem as shown in the textbook:\n",
    "\n",
    "* Consider driving a race car around a turn like those shown in Figure 5.5. You want to go as fast as possible, but not so fast as to run off the track. In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments to the velocity components. Each may be changed by +1, -1, or 0 in each step, for a total of nine (3 x 3) actions. Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line. Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line. The rewards are -1 for each step until the car crosses the finish line. If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car’s location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the finish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability 0.1 at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy (but turn the noise off for these trajectories).\n",
    "\n",
    "To complete this exercise, we first recognize that there are three parts involved:\n",
    "1. Setting up the environment\n",
    "2. Creating the car agent capable of MC control\n",
    "3. Train the agent and visualize outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be456ddb",
   "metadata": {},
   "source": [
    "### Setting Up the Environment\n",
    "\n",
    "We want to create a grid with a defined area that is our racetrack. We also want to define areas where the start and finish lines are. Anytime when the car agent is out of bound, it is returned to a random position in the start line and the episode continues. \n",
    "\n",
    "Below shows a skeleton of a track that is made to model one of the tracks shown in figure 5.5 in the textbook. There are a couple of options in terms of setting up the environment. You may choose to complete the Track class by filling in the functions intentionally left blank, you could toss the template code and build a track environment to your own likings, or you could use the track environment provided by the ```gym``` Python module. \n",
    "\n",
    "Note that you will have to install ```gym``` and import it to this lab if you choose to use ```gym```, and you will need to integrate the control algorithm we are building in this lab with the environment.\n",
    "\n",
    "\n",
    "#### Task 1: Complete the functions in the Track class\n",
    "\n",
    "\n",
    "The ```Track``` class is equipped with some basic functions such as setting up and visualizing a track. The learning agent can interact with the track to obtain information like: \n",
    "   * the state of the cell it is in\n",
    "        * the states are 'track', 'oob', 'start', 'finish'\n",
    "   * a random coordinate for a start state\n",
    "        * every time the agent goes off track, it will need to return to a randomized position on the starting line\n",
    "    * any other functions you feel would make sense as part of the environment (e.g. if the agent has crossed the finish line or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b563a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0196d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    \n",
    "  def __init__(self, _h, _w):\n",
    "    self.w = _w\n",
    "    self.h = _h\n",
    "    # set each cell in grid as 'track'\n",
    "    self.grid = np.array([['track'] * _w for _ in range(_h)], dtype='U8')\n",
    "\n",
    "  def make_track_1(self):\n",
    "    \"\"\"\n",
    "    sets the track on the left in figure 5.5 in textbook\n",
    "    \"\"\"\n",
    "    # set start line\n",
    "    self.grid[-1,:] = 'start'\n",
    "    # set finish line\n",
    "    self.grid[:,-1] = 'finish'\n",
    "    # set area to be out of bound\n",
    "    self.grid[:,0] = 'oob'\n",
    "    self.grid[0, :] = 'oob'\n",
    "    self.grid[:5,1] = 'oob'\n",
    "    self.grid[:4,2] = 'oob'\n",
    "    self.grid[:2,3] = 'oob'\n",
    "    self.grid[-18:,1] = 'oob'\n",
    "    self.grid[-10:,2] = 'oob'\n",
    "    self.grid[-7:,3] = 'oob'\n",
    "    self.grid[7:,11:] = 'oob'\n",
    "    self.grid[8:,10] = 'oob'\n",
    "    \n",
    "    # so that row 0 is start line\n",
    "    self.grid = np.flip(self.grid, axis=0)\n",
    "    \n",
    "  def show_grid(self, _show_trajectory=False, _ys=[], _xs=[]):\n",
    "    \"\"\"\n",
    "    visualize the track with cells in  \n",
    "    colors that coresponse to the cell type;\n",
    "    also plots a trajectory if provided with one\n",
    "    ---\n",
    "    Params:\n",
    "    _show_trajectory:  bool\n",
    "                       set to True if there is a trajectory to show\n",
    "    _ys: a numpy array containing the y values of the trajectory\n",
    "    _xs: a numpy array containing the x values of the trajectory\n",
    "    \"\"\"\n",
    "    flipped_grid = np.flip(self.grid, axis=0)\n",
    "    # setting up the colors that goes with cell type\n",
    "    self.color_dict = {'track': 'white', \n",
    "                       'oob': 'silver', \n",
    "                       'start': 'red', \n",
    "                       'finish': 'green'}\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(6, 10)\n",
    "    ax.set_xlim(-1/(self.w), self.w+1/(self.w))\n",
    "    ax.set_ylim(-1/(self.h), self.h+1/(self.h))\n",
    "    ax.set_axis_off()\n",
    "    # use the table and cell objects to make grid\n",
    "    table = Table(ax)\n",
    "    # table = Table(ax, bbox=[0, 0, 1, 1])\n",
    "    for (i, j), cell in np.ndenumerate(self.grid):\n",
    "        table.add_cell(\n",
    "            i, j, 1/self.w, 1/self.h, fill=True, \n",
    "            facecolor=self.color_dict[flipped_grid[i][j]]\n",
    "        )\n",
    "    ax.add_table(table)\n",
    "    if _show_trajectory:\n",
    "        xs = _xs + 0.5\n",
    "        ys = _ys + 0.5\n",
    "        plt.plot(xs, ys, '-')\n",
    "    plt.show()\n",
    "    \n",
    "  def get_cell_state(self, _y, _x):\n",
    "    \"\"\"\n",
    "    returns the state of the cell [_y, _x]\n",
    "            ('track', 'oob', 'start', 'finish')\n",
    "    \"\"\"\n",
    "    return 0\n",
    "    \n",
    "    \n",
    "  def randomized_start(self):\n",
    "    \"\"\"\n",
    "    this function should return a random point \n",
    "    on the starting line\n",
    "    \"\"\"\n",
    "    return 0   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abcd4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our environment\n",
    "H = 33\n",
    "W = 18\n",
    "track = Track(H, W)\n",
    "track.make_track_1()\n",
    "track.show_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a50f4b",
   "metadata": {},
   "source": [
    "### Creating the Car Agent\n",
    "\n",
    "We want to give the car agent access to neccessary information such as:\n",
    "* Its position on the grid\n",
    "* Its velocity\n",
    "* The availble actions and their constraints \n",
    "    * actions are +1, -1, and 0 in each direction on each step\n",
    "        * this translates to 9 actions: $[-1, -1], [-1, 0], [-1, 1], [0, -1], [0, 0], [0, 1], [1, -1], [1, 0], [1, -1]$\n",
    "        * determine the dimension of the state space and state-action space so that the matrices for Q, C, pi, etc are implemented correctly\n",
    "    * velocity components must be nonnegative and less than 5, and annot both be zero except at the starting line\n",
    "        * how would you handle checking the bound?\n",
    "        * if you had to correct the velocity because of the agent's action, do you record the action that was actually choosen or record the corrected action as if the agent knew about the constraint on its velocity? \n",
    "\n",
    "### Implementing the Learning Algorithm\n",
    "\n",
    "We can also implement off-policy Monte Carlo control algorithm in this next code block. For Monte Carlo, we want to document for each episode, for each time step $t$ the state $S_t$, the action taken $A_t$, and the reward the eagent received after transitioning to the next state $R_{t+1}$. We will only update $Q$ after the agent reaches the terminal state, in our case, the finish line.\n",
    "\n",
    "Since we want to implement an off-policy version of Monte Carlo, we also want to set up a policy matrix $\\pi$ so that we can iterate on it based on the learning from finishing each episode. Note that the agent will not be following $\\pi$ during the episode, and will instead follow a behavioral policy $b$, which is set to be any soft-policy in the textbook. Soft-policies can provide _coverage_, a property meaning that any actions taken under $\\pi$ will be taken under $b$ with a non-zero probability. \n",
    "\n",
    "Below is a template for the ```Agent``` class with a couple of suggested functions. Feel free to complete the functions, or write other functions to complete the neccessary structures for the learning process.\n",
    "\n",
    "\n",
    "#### Task 2: Complete the functions in the Agent class\n",
    "\n",
    "Below are some suggested functions if you would prefer more guidance; you can also get rid of the templates and start from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df7fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, _env, _eps=0.01, _gamma=0.9):\n",
    "        \n",
    "        self.env = _env\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.eps = _eps\n",
    "        self.gamma = _gamma\n",
    "        \n",
    "        # set up the dimension of different data structure for\n",
    "        # storing state-action values as tuples for easy accessing\n",
    "        self.q_dim = (???)\n",
    "        self.s_dim = (???)\n",
    "        \n",
    "        # if you don't want to set up the q_dim and s_dim, \n",
    "        # please change the line below to work with \n",
    "        # how you want to handle the initialization of q and c\n",
    "        self.q = np.full(self.q_dim, -1000) # arbitrary initialization value\n",
    "                                              # another arbitrary initialization: np.random.rand()*(1/len(self.actions))\n",
    "        # record keeping\n",
    "        self.c = np.zeros(self.q_dim) # used to track cummulative reward\n",
    "        \n",
    "        \n",
    "        # initialize a policy pi\n",
    "        # note that you will have to complete the generate_policy() function\n",
    "        self.pi = self.generate_policy() \n",
    "        \n",
    "        # set up storage for neccessary inforamtion such as \n",
    "        # agent's position, velocity, set of available actions, and more\n",
    "\n",
    "    \n",
    "    def generate_policy(self):\n",
    "        \"\"\"\n",
    "        generate pi based on Q\n",
    "        \"\"\"\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    def get_argmax(self):\n",
    "        \"\"\"\n",
    "        implement argmax for determine pi and possibly b \n",
    "        (depending on what kind of soft-policy b you end up implementing)\n",
    "        ---\n",
    "        returns the action with maximum expected value\n",
    "                \n",
    "        \"\"\"\n",
    "        return 0\n",
    "        \n",
    "\n",
    "    def find_possible_actions(self):\n",
    "        \"\"\"\n",
    "        gather actions that will not cause velocity  \n",
    "        to be more than 4, or negative, or both zeros\n",
    "        \"\"\"\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "    def epsilon_greedy(self):\n",
    "        \"\"\"\n",
    "        implement a soft behavioral policy b \n",
    "        using epsilon-greedy\n",
    "        ---\n",
    "        returns an action\n",
    "        \"\"\"\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    def get_next_pos(self):\n",
    "        \"\"\"\n",
    "        accepts an action that is a np array [y, x]\n",
    "        set the new position with the velocity\n",
    "        \"\"\"\n",
    "        return 0\n",
    "    \n",
    "                                \n",
    "    def get_new_v(self):\n",
    "        \"\"\"\n",
    "        get new velocity from  \n",
    "        current velocity and the choosen action\n",
    "        \"\"\"\n",
    "        return 0\n",
    "    \n",
    "                                \n",
    "    def mc_step(self):\n",
    "        \"\"\"\n",
    "        handles everything that needs to happen and store\n",
    "        in a single time step for Monte Carlo control\n",
    "        \"\"\"\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "    def offpolicy_mc_control(self, _episode=1000):\n",
    "        \"\"\"\n",
    "        generates episodes and iteratively updates policy \n",
    "        \"\"\"\n",
    "        for episode in range(_episode):\n",
    "            # loop self.mc_step() until terminal\n",
    "            # update Q and pi\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e12ec",
   "metadata": {},
   "source": [
    "## Traning with Monte Carlo\n",
    "\n",
    "Now that you have set up the agent and the environment, train the agent. Play around with the hyperparameters and the number of episode to see how they impact performance.\n",
    "\n",
    "You can visualize the agent's trajectory by using the ```show_grid()``` function provided in the ```Track``` class. Set the parameter ```_show_trajectory``` to ```True```, and pass in the y components and the x components of the agent's position history with the parameters ```_ys``` and ```_xs```.\n",
    "\n",
    "There might be a lot of bugs in your code at first. A good benchmark is that the first episode typically takes about 1000-2000 time steps to complete, and the subsequent episode should take significantly less time steps to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170eab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your agent here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
